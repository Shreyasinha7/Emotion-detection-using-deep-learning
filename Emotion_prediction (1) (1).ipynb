{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f357cf4",
   "metadata": {},
   "source": [
    "# 1.  Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd00105",
   "metadata": {},
   "source": [
    "#### Importing Libraries\n",
    "\n",
    "This section imports necessary libraries for video processing (cv2), numerical operations (numpy), and loading the pre-trained model (tensorflow.keras.models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801521a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b8ff94",
   "metadata": {},
   "source": [
    "#### Loading the Pre-trained Model\n",
    "\n",
    "Here, we load a pre-trained emotion detection model. The model is stored in the file emotion_detection_model_96.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e69379-8078-4880-826d-f373829c13f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('emotion_detection_model_96.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852584d2",
   "metadata": {},
   "source": [
    "#### Loading the Haar Cascade Classifier for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d218f267-a43b-4490-8986-70452e69ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d7091",
   "metadata": {},
   "source": [
    "#### Preprocessing Function\n",
    "\n",
    "This function preprocesses each video frame by resizing, converting to grayscale, converting back to RGB, normalizing, and expanding dimensions to fit the model's input requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fea8874-858e-41c2-bbff-4142755eac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Resize frame to match model's expected sizing (48x48)\n",
    "    frame = cv2.resize(frame, (48, 48))\n",
    "    # Convert to grayscale (if needed)\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Convert grayscale to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame_gray, cv2.COLOR_GRAY2RGB)\n",
    "    # Normalize\n",
    "    frame_rgb = frame_rgb / 255.0\n",
    "    # Expand dimensions to match model's input shape\n",
    "    frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "    return frame_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ded3c5",
   "metadata": {},
   "source": [
    "# 2. Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab099e7",
   "metadata": {},
   "source": [
    "#### Processing Video File\n",
    "\n",
    "This section reads the video file frame-by-frame, detects faces using the Haar Cascade classifier, preprocesses the face regions, performs emotion predictions using the pre-trained model, and displays the results with bounding boxes and emotion labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe95b5-bc1b-4088-8570-51c9844a71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'emotion_data.mp4'  # Replace with your video file path\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Loop through frames\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to grayscale for face detection\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(frame_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Loop over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region from the frame\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Preprocess the face region for emotion prediction\n",
    "        processed_face = preprocess_frame(face_roi)\n",
    "        \n",
    "        # Perform emotion prediction\n",
    "        prediction = model.predict(processed_face)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        emotion_label = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise'][predicted_class]\n",
    "        \n",
    "        # Overlay bounding box and emotion label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f'{emotion_label}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "    \n",
    "    # Display the frame with bounding boxes and emotion labels\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    \n",
    "    # Introduce a delay (1 millisecond here, adjust as needed)\n",
    "    # Press 'q' to exit or adjust the delay for slower playback\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b288e51",
   "metadata": {},
   "source": [
    "# 3. Data Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67be852",
   "metadata": {},
   "source": [
    "#### Processing Webcam Video\n",
    "\n",
    "This section captures video from the webcam, processes each frame to detect faces, performs emotion predictions, and displays the results. The main difference from the previous section is the source of video input (webcam vs. video file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2480dd0-542a-4269-bfac-ec260981e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # Use 0 for webcam, or replace with your video device index\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not access webcam or video device.\")\n",
    "    exit()\n",
    "\n",
    "# Loop to capture frames from the webcam\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture frame.\")\n",
    "        break\n",
    "    \n",
    "    # Convert frame to grayscale for face detection\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(frame_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    # Loop over detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region from the frame\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Preprocess the face region for emotion prediction\n",
    "        processed_face = preprocess_frame(face_roi)\n",
    "        \n",
    "        # Perform emotion prediction\n",
    "        prediction = model.predict(processed_face)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        emotion_label = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise'][predicted_class]\n",
    "        \n",
    "        # Overlay bounding box and emotion label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f'{emotion_label}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "    \n",
    "    # Display the frame with bounding boxes and emotion labels\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    \n",
    "    # Check for 'q' key press to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release webcam or capture device and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59dbc1",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrates how to preprocess video frames, detect faces, predict emotions using a pre-trained model, and visualize the results in real-time. It includes code for processing both video files and live webcam feeds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
